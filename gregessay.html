<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Welcome to Our Project</title>
  
  <!-- Link to external CSS -->
  <link rel= "stylesheet" href= "styles/stylesheets.css">

  <!-- top bar with links to other parts of the website -->
  <header>
    <nav>
      <ul class="nav-links">
        <li><a href="index.html">Home</a></li>
        <li><a href="about.html">About Us</a></li>
        <li><a href="bias-resources.html">Bias Resources</a></li>
        <li><a href="tech-hero.html">Tech Heros</a></li>
	<li><a href="ai.html">Teachable Machine</a></li>
	<li><a href="gregessay.html">Greg's Project Statement</a></li>
	<li><a href="jackessay.html">Jacks's Project Statement</a></li>




      </ul>
    </nav>
  </header>

  <!-- this is the information on the splash page to introduce the topics-->
  <section class="splash">
    <div class="container">
      <h1>Greg's Project Statement</h1>
      <h3>Introduction</h3>
      		<p>After reading Joy Buolamwini's 'Unmasking AI', my first thoughts were concern for the state of AI and the 
	 	hope for the future of the technology. My concern came from how widespread algorithmic bias remains 
		in the technologies I use daily.  The hope from the book was that her experiment with AI took place 
		during her PHD research in 2017, as well as her activism and awareness of the subject had helped 
		change how this technology operates. One specific part of her book was that the facial recognition 
		software she was using did not recognize her face; however, it did recognize her face when she wore 
		a white mask. This pivotal moment in her story was when she had to cover her ethnicity to be 'seen' 
		by facial recognition. Upon realizing that AI could not recognize her due to her skin complexion 
		revealed that those tools are not unbiased and project the same racial biases and hierarchy that 
		the creators of the technology have. As a white man, I have not experienced this type of racial 
		discrimination and technological disadvantages that Joy faces. This made me want to test whether 
		the biases that Buolamwini experienced during her PHD have improved and adapted to the criticisms 
		that happened during her experiment. Using Google's Teachable Machine, I created an image 
		classification model to detect whether a face was showing or not. This project attempts to see 
		whether skin tone is still a factor in the facial recognition software and if Joy's activism had 
		made an impact on this Machine Learning Software.
</p>
	<h3>Backround</h3>
      		<p>Joy Buolamwini describes how algorithmic systems can mirror societal biases.  She defines this 
		as the 'coded gaze' and that the people who create this software can create these biases. This is 
		significant as on page 11 of her novel, she states that "Predictive Ai systems are already used to 
		determine who gets a mortgage, who gets hired, who gets admitted to college, and who gets medical 
		treatment-products like ChatGPT have brought AI to new levels of public engagement and awareness." 
		AI has already seeped into every aspect of life, and if these coded biases remain in these systems,
		 that means that only white people will receive the benefits of these systems, and racial minorities
		 will continue to be discriminated against by these algorithms. Similarly to Joy, Ruha Benjiam 
		talks about the concept of "New Jim Code," referencing how that algorithmic decision-making can 
		perpetuate racial hierarchies such as those from the Jim Crow Era. My experiment builds on these 
		works in hopes of attempting to expose how Machine Learning and AI are ingrained with bias and need
		 to be used carefully.

</p>
	<h3>Experiment</h3>
      		<p>For this experiment,  I used Google's Teachable Machine, which allows the user to train the 
		software to recognize images, sounds, and poses. I used the image training section to recognize 
		faces similar to the software that Joy used in her experiment, as well as to determine if it does 
		not detects a face. Once the software is trained, I printed two headshots, one of a white male and 
		another of a black male. The images were then shown to the software at distances of one, three, 
		and ten feet. The percentage of the face detected is the quantitative data that I recorded. I 
		attempted to keep the other variables the same by using the same standard piece of paper, same 
		background for each image, as well as on the taped on the same chair to avoid potential biases 
		within my experiment. Although my experiment is rudimentary and outside variables may have been 
		impossible to avoid, the results speak for my shortcomings. 

</p>
	<h3>Results</h3>
      		<p>After the experiment, the findings show a clear bias towards the man with a darker skin 
		complexion. At both one foot and three feet, the model did not recognize the black man with more 
		confidence than the white man. Witnessing these descriptions in real time was sickening as it 
		nearly mirrored the experiences that Joy faced nearly seven years ago. The results highlight how 
		easily and stealthily these biases can fly under the radar and be built into software that is 
		life-altering. While some tech companies have claimed to improve the fairness and reduce biases 
		in their AI, it is now apparent that their efforts have not had the desired effect. Even though 
		this experiment was not up to standard for publishing, it still serves as a reminder that 
		algorithmic bias exacerbates systematic racism and continues the cycle of disempowering minorities.
 
</p>

	<h3>Connecting Theory</h3>
	<p>
	Creating and implementing this project has brought critical reflection on my privilege, especially the 
	privilege of being recognized and advantaged by AI and Machine Learning systems. As a white man, I thought 
	that technology would not recognize my presence nor well as be biased towards my race. The fact that the 
	white face was recognized better at every distance, and at three feet, was recognized twelve times better. 
	This familiar pattern is seen far too often, where people of color are consistently disadvantaged. 
	The experiment highlighted how algorithmic bias does not emerge by accident, it is embedded in the code and 
	foundation of these tools. Some of the causes are the lack of diversity in training data, unchecked 
	assumptions, and the lack of people from different backgrounds working on these projects, which always 
	biases into frame. These biases may be unintentional, however, their consequences are not. According to 
	<a href="https://hai.stanford.edu/news/ai-detectors-biased-against-non-native-english-writers" target="_blank">Stanford University</a>, 
	AI plagiarism checkers such as TurnItIn have a near-perfect score with native English speakers, however, 
	people who are not native speakers of English were flagged 61.22% incorrectly for using generative AI. 
	This experiment shows how tools like AI are a consequence of humans, and are political, biased, and 
	continue systemic racism that has been in place for centuries. When people are not recognized by these 
	systems, job opportunities, medical care, and legal representation, their identity and lives are being 
	erased. Being unseen by these systems is dehumanizing and demanding to those affected. Through this 
	experience, it has been shown that the only way to make AI more fair is to require inclusive design and 
	commit to having more justice at every stage of development.

</p>

	<h3>Future Work</h3>
	<p>
	AI is becoming increasingly embedded in critical areas, such as hiring, education, health care, and work. 
	The presence of bias in these systems is causing life-altering consequences. My experiment carries little 
	weight in the broader world of fairness, representation, and accountability of these new AI and Machine 
	Learning tools. The inability of Machine Learning to recognize certain faces accurately speaks volumes for 
	those who are marginalized daily. Moving forward, the solution is that we need more diverse and 
	representative training data and greater transparency on how AI is trained in development. Developers need 
	to include a wider range of people and prioritize ethics, and advocate for the Algorithmic Accountability 
	Act. At the end, we must ask what we want for AI and Machine Learning in the future? We must demand 
	corporate accountability and fairness in AI and Machine Learning 
</p>

	<h3>Conclusion</h3>
	<p>
	Creating this project fostered my understanding that technology is based on and shaped by human thoughts 
	and actions. Algorithmic injustice shapes our world, and the work of Joy Buolamwini, Ruha Bejamin, and 
	others reminds these systems must be held accountable and have a focus on ethics and equality. Continuing 
	this work matters more than ever as AI continues to gain popularity and shapes polices, identities, and 
	opportunities. AI must have transparency, inclusion, and collaboration so that we can build technologies 
	that serve fairly and fully to all.
	</p>

    </section>
</body>
	
<footer class="footer">
<p>Greg Delain & Jack Stecker | LIS 500</p>
</footer>

</html>
