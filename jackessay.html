<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Welcome to Our Project</title>
  
  <!-- Link to external CSS -->
  <link rel= "stylesheet" href= "styles/stylesheets.css">

  <!-- top bar with links to other parts of the website -->
  <header>
    <nav>
      <ul class="nav-links">
        <li><a href="index.html">Home</a></li>
        <li><a href="about.html">About Us</a></li>
        <li><a href="bias-resources.html">Bias Resources</a></li>
        <li><a href="tech-hero.html">Tech Heros</a></li>
	<li><a href="ai.html">Teachable Machine</a></li>
	<li><a href="gregessay.html">Greg's Project Statement</a></li>
	<li><a href="jackessay.html">Jacks's Project Statement</a></li>




      </ul>
    </nav>
  </header>

  <!-- this is the information on the splash page to introduce the topics-->
  <section class="splash">
    <div class="container">
      <h1>Jack's Project Statement</h1>
      <p>My time working on this project to create an AI model was an exciting and challenging experience. As someone who had limited experience and knowledge how AI models work and their interconnection between different social categories like race, gender, and class, it was humbling to see the underbelly of AI tools and the implicit biases that come with creating new technologies. After reading ‘Unmasking AI’ by Joy Buolawmini, respected computer scientist, and learning about her research about the unconscious attitudes and beliefs that are influencing our unintentional behaviors and interactions with the world, we see how the evidence points towards coded discrimination in tech products.</p>
      <p>Learning about Buolawmini’s facial recognition tools allowed me to become more aware of implicit biases and put into perspective the damages that are being done to marginalized groups. Before watching the documentary “Coded Bias” and reading the preview to her book, I had multiple schools of thought when I began to think about why certain faces were being detected more than other. I was skeptical at first but in hindsight I was ignorant. Seeing her use the machine learning tool go to work made me think how could this possibly be related to any biases towards and against anyone. I thought that there were technical errors that were happening and that’s why the results skewed one direction, but it was made clear that any technical challenges were likely delt with prior to finishing the machine and given institutions like MIT have some of the brightest minds in the world, especially PHD candidates like Joy, and perform rigorous testing procedures that may help identify technical errors and distinguish them from any patterns. And so, after watching Joy perform her machine on the white mask and her face multiple times again, I had come to realize and humbly accept that this reflected the data and algorithms used to teach the systems and an extension of the pre-existing social structures that determine how and what data is being used.</p>
      <p>We loosely based our machine algorithm on the same facial recognition tool we learned from Joy’s experiment while she was researching at MIT, and I felt a strong connection towards her efforts and findings. For our algorithm, we used Google’s Teachable Machine and chose it to train images that when all said and done, it would be able to detect facial patterns and then come to conclusion if there is a face on the screen or not. The process itself was a bit tedious where we tested images at three different lengths from the camera. To get the best results, we attempted to keep each test as similar as the last one so we could hash out any outliers. And what we found from the results was shocking. It was clear that after testing a light-skinned face versus a dark-skinned face, the results showed a bias against dark-skinned individuals where at two shortest lengths we tested, it recognized the dark-skinned face less than light-skinned faces. Our shortest length test was almost identical where there was a one percent difference favoring the light-skinned face and our longest test was the same both had zero percent deductions. However, our middle length test was the most jarring where our light skinned face came in at twenty-four percent detected versus our dark-skinned face coming in at only two percent. An astounding twenty-two percent difference at a regular distance. Seeing these numbers made me think how similarly it resembled the findings from Joy’s research and the reality of how easily certain biases can sneak their way into making these technologies and algorithms. It’s alarming to see a susceptible a basic algorithm like ours was to biases. This certainly made me reflect on the significance of this experiment and the damages that can be done in real workplaces and environments that are affecting not just different races, but all the genders, orientations, and social classes too. This was truly a reminder and assurance that the implicit biases are here. And as a white male lucky enough to be put in position to succeed, this is a friendly reminder to take responsibility for advocating for the unrepresented and equitable fairness. Algorithmic bias and systemic issues need to be addressed before it damages communities even more.</p>
      <p>Intersectionality is a complex way of thinking of how some social classes often produce greater inequalities in systems that favor forms of structural racism and form hierarchies. The findings from this project showed me the harsh realities of the world we live in. As we strive to find fairness and equitability in our communities, we must come together to continue to learn and share more about diversity. Ultimately, addressing these issues by creating comprehensive frameworks in companies, between clients, and in communities moving forward can put help promote justice. Whether we may not see how technology can impact marginalized communities, we must continue to ask the tough questions and advocate for systems that support all social groups.</p>
    </section>
</body>
	
<footer class="footer">
<p>Greg Delain & Jack Stecker | LIS 500</p>
</footer>

</html>
